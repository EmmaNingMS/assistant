{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdce572-02eb-421e-a1c2-d169a4c85e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxruntime-genai\n",
    "!pip install onnxruntime\n",
    "!pip install olive\n",
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea1db1-4460-49ed-8da5-918a1adb460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token <TOKEN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e86b3453-0e9f-47fb-ab63-193e0e06c583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-10 18:59:28,424] [INFO] [run.py:243:run] Loading Olive module configuration from: C:\\Users\\nakersha\\AppData\\Local\\miniconda3\\envs\\assistant\\Lib\\site-packages\\olive\\olive_config.json\n",
      "[2024-04-10 18:59:28,430] [INFO] [run.py:249:run] Loading run configuration from: olive.json\n",
      "[2024-04-10 18:59:28,438] [INFO] [config.py:184:validate_evaluate_input_model] No evaluator is specified, skip to evaluate model\n",
      "[2024-04-10 18:59:28,443] [DEBUG] [accelerator.py:245:normalize_accelerators] The accelerator device and execution providers are specified, skipping deduce.\n",
      "[2024-04-10 18:59:28,445] [DEBUG] [accelerator.py:275:normalize_accelerators] Supported execution providers for device cpu: ['CPUExecutionProvider']\n",
      "[2024-04-10 18:59:28,448] [DEBUG] [accelerator.py:302:create_accelerators] Initial accelerators and execution providers: {'CPU': ['CPUExecutionProvider']}\n",
      "[2024-04-10 18:59:28,451] [INFO] [accelerator.py:324:create_accelerators] Running workflow on accelerator specs: cpu-cpu\n",
      "[2024-04-10 18:59:28,454] [INFO] [run.py:196:run_engine] Importing pass module GenAIModelExporter\n",
      "[2024-04-10 18:59:28,457] [INFO] [engine.py:106:initialize] Using cache directory: cache\n",
      "[2024-04-10 18:59:28,464] [INFO] [engine.py:262:run] Running Olive on accelerator: cpu-cpu\n",
      "[2024-04-10 18:59:28,465] [DEBUG] [engine.py:1070:create_system] create native OliveSystem SystemType.Local\n",
      "[2024-04-10 18:59:28,467] [DEBUG] [engine.py:1070:create_system] create native OliveSystem SystemType.Local\n",
      "[2024-04-10 18:59:28,474] [DEBUG] [engine.py:708:_cache_model] Cached model 1473a6e460df1ddcd4cf088ff0019b1e to cache\\models\\1473a6e460df1ddcd4cf088ff0019b1e.json\n",
      "[2024-04-10 18:59:28,477] [DEBUG] [engine.py:335:run_accelerator] Running Olive in no-search mode ...\n",
      "[2024-04-10 18:59:28,478] [DEBUG] [engine.py:427:run_no_search] Running ['genai_exporter'] with no search ...\n",
      "[2024-04-10 18:59:28,479] [INFO] [engine.py:864:_run_pass] Running pass genai_exporter:GenAIModelExporter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d3451b424148e1a06ba27c9b42b838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embedding layer\n",
      "Reading decoder layer 0\n",
      "Reading decoder layer 1\n",
      "Reading decoder layer 2\n",
      "Reading decoder layer 3\n",
      "Reading decoder layer 4\n",
      "Reading decoder layer 5\n",
      "Reading decoder layer 6\n",
      "Reading decoder layer 7\n",
      "Reading decoder layer 8\n",
      "Reading decoder layer 9\n",
      "Reading decoder layer 10\n",
      "Reading decoder layer 11\n",
      "Reading decoder layer 12\n",
      "Reading decoder layer 13\n",
      "Reading decoder layer 14\n",
      "Reading decoder layer 15\n",
      "Reading decoder layer 16\n",
      "Reading decoder layer 17\n",
      "Reading decoder layer 18\n",
      "Reading decoder layer 19\n",
      "Reading decoder layer 20\n",
      "Reading decoder layer 21\n",
      "Reading decoder layer 22\n",
      "Reading decoder layer 23\n",
      "Reading decoder layer 24\n",
      "Reading decoder layer 25\n",
      "Reading decoder layer 26\n",
      "Reading decoder layer 27\n",
      "Reading decoder layer 28\n",
      "Reading decoder layer 29\n",
      "Reading decoder layer 30\n",
      "Reading decoder layer 31\n",
      "Reading final norm\n",
      "Reading LM head\n",
      "Saving ONNX model in C:\\Users\\nakersha\\Develop\\code\\natke\\assistant\\cache\\models\\0_GenAIModelExporter-1473a6e460df1ddcd4cf088ff0019b1e-fe48ab55cdf4d03b843ede7c3c3be27b-cpu-cpu\\output_model\n",
      "Saving GenAI config in C:\\Users\\nakersha\\Develop\\code\\natke\\assistant\\cache\\models\\0_GenAIModelExporter-1473a6e460df1ddcd4cf088ff0019b1e-fe48ab55cdf4d03b843ede7c3c3be27b-cpu-cpu\\output_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processing files in C:\\Users\\nakersha\\Develop\\code\\natke\\assistant\\cache\\models\\0_GenAIModelExporter-1473a6e460df1ddcd4cf088ff0019b1e-fe48ab55cdf4d03b843ede7c3c3be27b-cpu-cpu\\output_model for GenAI\n",
      "[2024-04-10 19:05:32,649] [DEBUG] [resource_path.py:156:create_resource_path] Resource path C:\\Users\\nakersha\\Develop\\code\\natke\\assistant\\cache\\models\\0_GenAIModelExporter-1473a6e460df1ddcd4cf088ff0019b1e-fe48ab55cdf4d03b843ede7c3c3be27b-cpu-cpu\\output_model is inferred to be of type folder.\n",
      "[2024-04-10 19:05:32,672] [INFO] [engine.py:951:_run_pass] Pass genai_exporter:GenAIModelExporter finished in 364.187977 seconds\n",
      "[2024-04-10 19:05:32,679] [DEBUG] [engine.py:708:_cache_model] Cached model 0_GenAIModelExporter-1473a6e460df1ddcd4cf088ff0019b1e-fe48ab55cdf4d03b843ede7c3c3be27b-cpu-cpu to cache\\models\\0_GenAIModelExporter-1473a6e460df1ddcd4cf088ff0019b1e-fe48ab55cdf4d03b843ede7c3c3be27b-cpu-cpu.json\n",
      "[2024-04-10 19:05:32,685] [DEBUG] [engine.py:791:_cache_run] Cached run for 1473a6e460df1ddcd4cf088ff0019b1e->0_GenAIModelExporter-1473a6e460df1ddcd4cf088ff0019b1e-fe48ab55cdf4d03b843ede7c3c3be27b-cpu-cpu into cache\\runs\\GenAIModelExporter-1473a6e460df1ddcd4cf088ff0019b1e-fe48ab55cdf4d03b843ede7c3c3be27b-cpu-cpu.json\n",
      "[2024-04-10 19:05:32,691] [DEBUG] [engine.py:844:_run_passes] Signal: None\n",
      "[2024-04-10 19:05:32,722] [DEBUG] [resource_path.py:156:create_resource_path] Resource path C:\\Users\\nakersha\\Develop\\code\\natke\\assistant\\cache\\models\\0_GenAIModelExporter-1473a6e460df1ddcd4cf088ff0019b1e-fe48ab55cdf4d03b843ede7c3c3be27b-cpu-cpu\\output_model is inferred to be of type folder.\n",
      "[2024-04-10 19:05:50,888] [INFO] [engine.py:361:run_accelerator] Save footprint to example-models\\microsoft\\phi2-cpu-int4\\cpu-cpu_footprints.json.\n",
      "[2024-04-10 19:05:50,891] [DEBUG] [engine.py:363:run_accelerator] run_accelerator done\n",
      "[2024-04-10 19:05:50,892] [INFO] [engine.py:279:run] Run history for cpu-cpu:\n",
      "[2024-04-10 19:05:50,900] [INFO] [engine.py:569:dump_run_history] Please install tabulate for better run history output\n",
      "[2024-04-10 19:05:50,906] [INFO] [engine.py:294:run] No packaging config provided, skip packaging artifacts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{AcceleratorSpec(accelerator_type=<Device.CPU: 'cpu'>, execution_provider='CPUExecutionProvider', vender=None, version=None, memory=None, num_cores=None): <olive.engine.footprint.Footprint at 0x24294f50050>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import olive.workflows\n",
    "\n",
    "olive.workflows.run(\"olive.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "569d2618-ff32-466a-8bec-eeb967ee364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "add45ace-14be-4ab3-a68c-303aebeea18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded in 36.07 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "app_started_timestamp = time.time()\n",
    "\n",
    "model = og.Model(f'example-models\\microsoft\\phi2-cpu-int4\\genai_exporter\\cpu-cpu_model')\n",
    "model_loaded_timestamp  = time.time()\n",
    "\n",
    "print(\"Model loaded in {:.2f} seconds\".format(model_loaded_timestamp - app_started_timestamp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79513969-40bc-4588-a10c-8c482d224fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Tokenizer created\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()\n",
    "\n",
    "print(\"Tokenizer created\")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant. Answer in one sentence.\"\n",
    "text = \"What is Dilithium?\"\n",
    "\n",
    "input_tokens = tokenizer.encode(system_prompt + text)\n",
    "\n",
    "prompt_length = len(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dcf8cc3-d5d2-42b1-8ad1-76d6629667b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating generator ...\n",
      "Generator created\n",
      "\n",
      "A: Dilithium is a fictional substance in the Star Trek universe that is used as a propellant and a power source for spaceships.\n",
      "\n",
      "Prompt tokens: 17, New tokens: 32, Time to first: 1.09s, New tokens per second: 3.83 tps\n"
     ]
    }
   ],
   "source": [
    "started_timestamp = time.time()\n",
    "\n",
    "print(\"Creating generator ...\")\n",
    "params = og.GeneratorParams(model)\n",
    "params.set_search_options({\"do_sample\": False, \"max_length\": 2028, \"min_length\": 0, \"top_p\": 0.9, \"top_k\": 40, \"temperature\": 1.0, \"repetition_penalty\": 1.0})\n",
    "params.input_ids = input_tokens\n",
    "generator = og.Generator(model, params)\n",
    "print(\"Generator created\")\n",
    "\n",
    "first = True\n",
    "new_tokens = []\n",
    "\n",
    "while not generator.is_done():\n",
    "    generator.compute_logits()\n",
    "    generator.generate_next_token()\n",
    "    if first:\n",
    "        first_token_timestamp = time.time()\n",
    "        first = False\n",
    "\n",
    "    new_token = generator.get_next_tokens()[0]\n",
    "    print(tokenizer_stream.decode(new_token), end=\"\")\n",
    "    new_tokens.append(new_token)\n",
    "\n",
    "print()\n",
    "run_time = time.time() - started_timestamp\n",
    "print(f\"Prompt tokens: {len(input_tokens)}, New tokens: {len(new_tokens)}, Time to first: {(first_token_timestamp - started_timestamp):.2f}s, New tokens per second: {len(new_tokens)/run_time:.2f} tps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8c452-037b-4566-8f81-b0e2c8a57c6c",
   "metadata": {},
   "source": [
    "## Download and build llama.cpp\n",
    "* git clone https://github.com/ggerganov/llama.cpp.git\n",
    "* cd llama.cpp\n",
    "* cmake -S . -B build/ -D CMAKE_BUILD_TYPE=Release\n",
    "* cmake --build build/ --config Release\n",
    "\n",
    "## Download gguf phi-2 model\n",
    "* cd ..\n",
    "* git clone https://huggingface.co/TheBloke/phi-2-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd4e897-1316-4f80-8fe1-0088341be5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant. Answer in one sentence. What is Dilithium?\n",
      "A: Dilithium is a powerful explosive used in the Star Trek universe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 2647 (8228b66d)\n",
      "main: built with MSVC 19.39.33523.0 for x64\n",
      "main: seed  = 1712797839\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 325 tensors from ..\\..\\thebloke\\phi-2-GGUF\\phi-2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi2\n",
      "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 10240\n",
      "llama_model_loader: - kv   5:                           phi2.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ä  t\", \"Ä  a\", \"h e\", \"i n\", \"r e\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  195 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 910/51200 vs 944/51200 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 51200\n",
      "llm_load_print_meta: n_merges         = 50000\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2560\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 32\n",
      "llm_load_print_meta: n_embd_head_k    = 80\n",
      "llm_load_print_meta: n_embd_head_v    = 80\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2560\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2560\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 10240\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 2.78 B\n",
      "llm_load_print_meta: model size       = 1.66 GiB (5.14 BPW) \n",
      "llm_load_print_meta: general.name     = Phi2\n",
      "llm_load_print_meta: BOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 50256 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ã„'\n",
      "llm_load_tensors: ggml ctx size =    0.12 MiB\n",
      "llm_load_tensors:        CPU buffer size =  1704.63 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   160.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  160.00 MiB, K (f16):   80.00 MiB, V (f16):   80.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.20 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   105.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1225\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "\n",
      "system_info: n_threads = 6 / 12 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 2048, n_predict = -1, n_keep = 0\n",
      "\n",
      "\n",
      " [end of text]\n",
      "\n",
      "llama_print_timings:        load time =    1380.91 ms\n",
      "llama_print_timings:      sample time =       0.94 ms /    19 runs   (    0.05 ms per token, 20277.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     878.76 ms /    17 tokens (   51.69 ms per token,    19.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1546.56 ms /    18 runs   (   85.92 ms per token,    11.64 tokens per second)\n",
      "llama_print_timings:       total time =    2434.14 ms /    35 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "# Compare with llama.cpp.\n",
    "\n",
    "! ..\\..\\ggerganov\\llama.cpp\\build\\bin\\Release\\main -m ..\\..\\the\\phi-2-GGUF\\phi-2.Q4_K_M.gguf --prompt \"You are a helpful assistant. Answer in one sentence. What is Dilithium?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2672d-2f25-4f89-9d74-d87c37f453d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
